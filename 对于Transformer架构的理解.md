
<img width="1141" height="1200" alt="image" src="https://github.com/user-attachments/assets/591c0a0d-c4e3-4fcb-b0a8-af37ff7a4d50" />

网络上有太多对Attention is All You Need这篇论文的nb讲解了，我基本也是参照着这些导读文章和视频才摸明白的（为什么学校AI课还在教老东西啊。。。CS171 CS178说的就是你俩）我写这篇是主要是为了遵循费曼学习法写来玩的！

这张图其实足够简洁清晰地让我在不记得的时候重温这个架构。首先这个架构分为两边，分别是输入和输出。左边是输入层，训练时候的语料输入和推理时候的用户输入都在这边。输入层的作用就是了解清楚输入的这句话究竟是什么意思。比方说：“今早我去买了本书，然后去了趟奶茶店买了杯糯香柠檬茶。晚上去图书馆，把它看完了，看的途中在那儿把它喝完了。” 传统铸币模型是无法分辨后面的两个它具体代表什么意思的，这里的输入层就会通过一系列操作，让模型理解这句话里面的第一个它代表的是书，第二个它代表的是柠檬茶。在模型理解之后，后边的输出层就可以发挥作用了。它会根据理解到的tokens和tokens之间的关系，来预测下一个token输入什么的概率最高。这就是输出层的作用。两边里面的具体细节在下文开始讨论（也没多细，细的我也不懂）

我们首先看输入层source sequence也就是输入的东西，输入进来之后机器是不懂的呀！你给一台机器说“今早我去买了本书”，它只会觉得你叽里咕噜在说啥呢，最主要是无法进行后面的数学运算，所以我们要转成可以运算且机器看得懂的东西。那首先我们要把输入的sequence拆分成一个个的tokens，例如“今早我去买了本书” → “今早 我 去买了 本 书”（具体如何tokenize的是上级有关部门决定的）然后要给每一个token一个id，方便后面模型自己知道自己在说哪个词。然后，模型会把这个token在embedding矩阵中取出来，取出来的是一个512维的向量。512维向量中，每个维度代表了一个属性。越接近的属性的向量，在512维的坐标系里面就离得越近。这里举个例子，每个人有四个向量，年龄，身高，体重，性别，假如小明的年龄15，身高175cm，体重56kg，性别男，小帅年龄15，身高175cm，体重55kg，性别男，那这两个token（小明和小帅）的向量就很接近，就说明他们俩很相似。在transormer的实际过程中，512个向量是抽象的数学概念，并没有被定义好，但是总的来说两个向量相近的概念他们的语义是相似的。有一个很好的例子，中国 - 北京 的向量距离，可能会近似于 日本 - 东京 的向量距离，因为他们语义相似。这样子我们就成功让输入的东西也能被机器和模型读懂了。

接下来我们进行一层normalization，就进入了multi-headed self-attention，多头注意力机制！这里有三个重要参数, Q, K, V，分别代表Query, Key, Value。每个token都有自己的QKV。用非常直观的描述来表达这三个参数，K表示这个token在句子中代表的身份，Q是在问这个token想知道什么信息，V则是这个token的具体含义和客观信息。用前面的例子：“今早我去买了本书，然后去了趟奶茶店买了杯糯香柠檬茶。晚上去图书馆，把它看完了，看的途中在那儿把它喝完了。” 里面的“我”这个token的身份是这个句子的主语，也就是说”我“这个token的Key是”这个句子的主语“。而”我“这个token的Query则是”我做了什么事？我要干什么？“，而Value是“指代自己的第一人称代词”。

在多头注意力机制中，所有token的K都要和其他token的Q进行相乘，已确定相关性高不高。如果一个词的Q和另一个词的K互相匹配上了，那么它的embedding 向量就会往另一个词的V的方向偏移一些。举例，刚才例子中，第二个“它”的Key是“一个第三人称的代词，且可以被喝”，他的Queue是我指代了前面什么可以喝的东西？“，而”糯香柠檬茶“的Key是”一种奶茶的种类或名称“，那么这俩K和Q就对应上了，也就是“它”对于“糯香柠檬茶”的attention更高一些，于是“它”这个token的embedding向量就会稍微向“糯香柠檬茶”的向量进行偏移。

我们由此得到下面著名的attention公式。

<img width="379" height="80" alt="image1" src="https://github.com/user-attachments/assets/3e0c10a5-32a7-42e8-972d-a861668631c6" />


我们可以更直观的总结成这句话：

Attention：

> “我在用 Q 提问，用 K 找相关性，用 V 拿信息”
> 

公式拆解：

$QK^⊤$ 就是在给Q和K做点积，点积越大说明相关性越高

$QK^⊤ \over √dₖ$ 缩放一下，放置维度高的时候数值过大导致softmax不稳定

<aside>
💡

怕你忘了什么是softmax（其实是我忘了）softmax就是把所有数都做一个指数运算，然后让所有输出加起来等于1，也就是给每个数获得一个概率分布，让所有数都是非负数，且让加权稳定。softmax只关注相对距离，也就是“谁比谁更重要”。

</aside>

后面乘上的V其实就是这个attention里面的原始embedding向量经过了多次attention计算的结果，但是他不能改变全局的只能改这一步的。

这样就终于理解了Attention机制，以及Q，K，V的作用了（反正我应该理解了费曼跟我说的）

实际上在Multi-Headed Self-Attention里面，刚才说的这个计算过程一共进行了若干次（论文里面说是6次没记错的话），而每次计算出的Q都会着重于不同的东西上，这也就是为什么叫做Multi-Headed。

Multi-Headed Self-Attention这一步的最终结果就是模型为每个token都计算出了一个偏移量，这个偏移量表示了这个token结合上下文之后需要进行的调整。例如刚才例子中第一个“它”就向“书籍、书本”这个概念进行了偏移，第二个“它”向“糯香柠檬茶”进行偏移。而Add & Norm这步则是先把这些偏移量加回原始输入信息的向量，然后把他们调整回以坐标零点为中心的均值0，方差1的数据分布，让数据分布更加稳定，训练模型的时候更容易收敛。

当完成了Multi-Head Self-Attention的计算之后，我们做一个normalization，就进入了Feed Forward Network。这一步直白一点讲就是让每个token自己消化加工前面获得的信息。在self-attention层里面已经得到了跟其他token交流完之后获得的权重，现在token可以通过引入非线性来实现模型表达能力。具体步骤是，刚才说了一个token的维度是512，模型会先升维到2048维，把特征再次摊开，每个新维度都是原特征进行加权组合产生的，也就是某个维度的不同视角。然后使用ReLU或者GeLU等类似的非线性函数来筛选。最后，计算完的2048维再压缩回新的256维。

<aside>
💡

这里提一下MoE。DeepSeek V3当时就遥遥领先地在FFN这层采用了MoE来代替传统FFN。MoE(Mixures of Experts混合专家模型)总的来说就是一次性加载出非常多种不同的FFN计算专家，例如专门搞数学的，搞编程的，等等（只是方便理解，本质上都只是数学概念）。然后有个Router（门控）会判断这个刚刚从attention层输出的token应该要给到哪个专家来进行FFN的计算，这样的好处就是**在几乎不增加每 token 计算量的情况下，极大提升模型容量和专门化能力。**

</aside>

完成FFN层之后再做一个Normalization，至此输入层的计算就全部结束了。而输入层计算出来的每个词的K和V也会被当作输入交给输出层中用于推理，我们开始讲输出层。

输出层的目标就是根据问题的内容，逐字输出token。例如我们给到的输入是：“今天天气如何？“ 输出层会根据问题开始输出。由于前面还没有任何词输出，所以输出层输出的第一个词就是<start>。这第一个输出的token会经过masked self-attention，这个我们稍后讲。接下来，他会经过multi-headed cross-attention。这里接收到的Q就是刚才传进来的<start>，而V和K是刚才在输入层计算出来的根据上下文进行调整了的V矩阵和K矩阵。在这一步，模型就会这样思考：“ok第一个词是<start>，一般句子开头都是用主语来开头，那我要用什么主语来start呢？既然用户问的是今天天气如何，那我也要用‘今天’来开头。“于是模型就通过计算，得出了一个非常接近”今天“这个词组的向量。再把这个向量进行FFN（跟输入层的做法一模一样就不再解释了）

得到了这个计算出的向量之后，由linear层做一次softmax，把这个向量得到的所有可能性都变成一个概率分布，最后选出概率最高的token，也就是”今天“这个词（当然模型也有可能觉得今天开头太老土了，于是他就用”你好“来开头，这一切都取决于哪个的概率分布高）这样的过程循环往复，直到他认为应该要输出<end>为止。

那么训练模型具体是在训练什么呢？
TODO：2025.1.15 写到这里

Q, K, V分别都是模型在训练过程中反复调整的矩阵。

